{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning\n",
    "\n",
    "## Homework 8: Introduction to Computer vision, Time Series, and Survival Analysis (Lectures 19 to 20) \n",
    "\n",
    "**Due date: see the [Apr 07, 11:59 pm](https://github.com/UBC-CS/cpsc330-2024W2?tab=readme-ov-file#deliverable-due-dates-tentative).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W2/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset and storing it under the `data` folder. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "rubric={points:4}\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture demo, we had different measurements for each Location. \n",
    "\n",
    "We want you to consider this for the avocado prices dataset. For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date            datetime64[ns]\n",
       "AveragePrice           float64\n",
       "Total Volume           float64\n",
       "4046                   float64\n",
       "4225                   float64\n",
       "4770                   float64\n",
       "Total Bags             float64\n",
       "Small Bags             float64\n",
       "Large Bags             float64\n",
       "XLarge Bags            float64\n",
       "type                    object\n",
       "year                     int64\n",
       "region                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique regions: 54\n",
      "Number of unique avocado types: 2\n"
     ]
    }
   ],
   "source": [
    "num_regions = df[\"region\"].nunique()\n",
    "num_types = df[\"type\"].nunique()\n",
    "\n",
    "print(\"Number of unique regions:\", num_regions)\n",
    "print(\"Number of unique avocado types:\", num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby([\"region\", \"type\"]).size().reset_index(name=\"Count\")\n",
    "num_time_series = len(grouped)\n",
    "num_time_series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avocado prices dataset includes separate measurements for each unique region and type since the region column identifies distinct geographic areas and the type column distinguishes between conventional and organic avocados. The dataset has 54 unique regions and 2 unique types so that every combination of these features represents its own time series which result in 108 separate time series in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2015-02-08</td>\n",
       "      <td>0.99</td>\n",
       "      <td>51253.97</td>\n",
       "      <td>1357.37</td>\n",
       "      <td>39111.81</td>\n",
       "      <td>163.25</td>\n",
       "      <td>10621.54</td>\n",
       "      <td>10113.10</td>\n",
       "      <td>508.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2015-02-15</td>\n",
       "      <td>1.06</td>\n",
       "      <td>41567.62</td>\n",
       "      <td>986.66</td>\n",
       "      <td>30045.51</td>\n",
       "      <td>222.42</td>\n",
       "      <td>10313.03</td>\n",
       "      <td>9979.87</td>\n",
       "      <td>333.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2015-02-22</td>\n",
       "      <td>1.07</td>\n",
       "      <td>45675.05</td>\n",
       "      <td>1088.38</td>\n",
       "      <td>35056.13</td>\n",
       "      <td>151.00</td>\n",
       "      <td>9379.54</td>\n",
       "      <td>9000.16</td>\n",
       "      <td>379.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>55595.74</td>\n",
       "      <td>629.46</td>\n",
       "      <td>45633.34</td>\n",
       "      <td>181.49</td>\n",
       "      <td>9151.45</td>\n",
       "      <td>8986.06</td>\n",
       "      <td>165.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2015-03-08</td>\n",
       "      <td>1.07</td>\n",
       "      <td>40507.36</td>\n",
       "      <td>795.68</td>\n",
       "      <td>30370.64</td>\n",
       "      <td>159.05</td>\n",
       "      <td>9181.99</td>\n",
       "      <td>8827.55</td>\n",
       "      <td>354.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "51 2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "50 2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "49 2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "48 2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "47 2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "46 2015-02-08          0.99      51253.97  1357.37  39111.81  163.25   \n",
       "45 2015-02-15          1.06      41567.62   986.66  30045.51  222.42   \n",
       "44 2015-02-22          1.07      45675.05  1088.38  35056.13  151.00   \n",
       "43 2015-03-01          0.99      55595.74   629.46  45633.34  181.49   \n",
       "42 2015-03-08          1.07      40507.36   795.68  30370.64  159.05   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "50     8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "49    11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "48    10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "47     9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "46    10621.54    10113.10      508.44          0.0  conventional  2015   \n",
       "45    10313.03     9979.87      333.16          0.0  conventional  2015   \n",
       "44     9379.54     9000.16      379.38          0.0  conventional  2015   \n",
       "43     9151.45     8986.06      165.39          0.0  conventional  2015   \n",
       "42     9181.99     8827.55      354.44          0.0  conventional  2015   \n",
       "\n",
       "    region  \n",
       "51  Albany  \n",
       "50  Albany  \n",
       "49  Albany  \n",
       "48  Albany  \n",
       "47  Albany  \n",
       "46  Albany  \n",
       "45  Albany  \n",
       "44  Albany  \n",
       "43  Albany  \n",
       "42  Albany  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"region\", \"type\", \"Date\"]).head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset shows that each date is represented only once for every combination of region and type. This confirms that the measurements for each unique region and type pair are tracked independently over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: Albany - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Albany - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Atlanta - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Atlanta - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: BaltimoreWashington - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: BaltimoreWashington - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Boise - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Boise - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Boston - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Boston - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: BuffaloRochester - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: BuffaloRochester - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: California - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: California - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Charlotte - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Charlotte - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Chicago - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Chicago - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: CincinnatiDayton - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: CincinnatiDayton - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Columbus - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Columbus - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: DallasFtWorth - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: DallasFtWorth - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Denver - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Denver - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Detroit - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Detroit - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: GrandRapids - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: GrandRapids - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: GreatLakes - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: GreatLakes - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: HarrisburgScranton - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: HarrisburgScranton - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: HartfordSpringfield - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: HartfordSpringfield - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Houston - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Houston - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Indianapolis - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Indianapolis - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Jacksonville - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Jacksonville - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: LasVegas - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: LasVegas - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: LosAngeles - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: LosAngeles - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Louisville - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Louisville - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: MiamiFtLauderdale - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: MiamiFtLauderdale - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Midsouth - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Midsouth - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Nashville - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Nashville - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: NewOrleansMobile - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: NewOrleansMobile - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: NewYork - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: NewYork - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Northeast - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Northeast - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: NorthernNewEngland - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: NorthernNewEngland - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Orlando - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Orlando - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Philadelphia - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Philadelphia - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: PhoenixTucson - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: PhoenixTucson - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Pittsburgh - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Pittsburgh - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Plains - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Plains - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Portland - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Portland - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: RaleighGreensboro - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: RaleighGreensboro - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: RichmondNorfolk - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: RichmondNorfolk - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Roanoke - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Roanoke - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Sacramento - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Sacramento - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SanDiego - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SanDiego - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SanFrancisco - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SanFrancisco - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Seattle - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Seattle - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SouthCarolina - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SouthCarolina - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SouthCentral - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: SouthCentral - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Southeast - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Southeast - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Spokane - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Spokane - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: StLouis - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: StLouis - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Syracuse - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Syracuse - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Tampa - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: Tampa - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: TotalUS - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: TotalUS - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: West - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: West - organic | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: WestTexNewMexico - conventional | Min gap: 7 days 00:00:00 | Max gap: 7 days 00:00:00\n",
      "Group: WestTexNewMexico - organic | Min gap: 7 days 00:00:00 | Max gap: 21 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Group by region and type and print out\n",
    "for (region, avocado_type), group in df.groupby([\"region\", \"type\"]):\n",
    "    sorted_dates = group[\"Date\"].sort_values()\n",
    "    date_diff = sorted_dates.diff().dropna()\n",
    "    print(\"Group: {} - {} | Min gap: {} | Max gap: {}\".format(region, avocado_type, date_diff.min(), date_diff.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avocado dataset generally has equally spaced measurements on a weekly basis. Most region and type groups show both a minimum and maximum gap of exactly seven days between consecutive dates, which indicates consistent weekly intervals. However, there is an exception: the WestTexNewMexico region with organic avocados where the maximum gap is 21 days. This suggests that although the dataset is almost always recorded weekly, some measurements may be missing or delayed in a few cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "7 days     163\n",
       "14 days      1\n",
       "21 days      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for WestTexNewMexico organic avocados\n",
    "west_tex_org = df[(df[\"region\"] == \"WestTexNewMexico\") & (df[\"type\"] == \"organic\")].sort_values(\"Date\")\n",
    "diffs = west_tex_org[\"Date\"].diff().dropna().value_counts()\n",
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date difference frequencies for Albany, conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For a single group, check how frequently each time gap occurs\n",
    "sample_group = df[\n",
    "    (df[\"region\"] == \"Albany\") &\n",
    "    (df[\"type\"] == \"conventional\")\n",
    "].sort_values(\"Date\")\n",
    "date_diffs = sample_group[\"Date\"].diff().dropna()\n",
    "print(\"Date difference frequencies for Albany, conventional:\")\n",
    "print(date_diffs.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the example above, Albany conventional avocados consistently exhibit a 7-day gap. In contrast, WestTexNewMexico organic avocados occasionally jump by 14 or 21 days. This confirms that the measurements are almost always equally spaced but with a few exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique regions: ['Albany' 'Atlanta' 'BaltimoreWashington' 'Boise' 'Boston'\n",
      " 'BuffaloRochester' 'California' 'Charlotte' 'Chicago' 'CincinnatiDayton'\n",
      " 'Columbus' 'DallasFtWorth' 'Denver' 'Detroit' 'GrandRapids' 'GreatLakes'\n",
      " 'HarrisburgScranton' 'HartfordSpringfield' 'Houston' 'Indianapolis'\n",
      " 'Jacksonville' 'LasVegas' 'LosAngeles' 'Louisville' 'MiamiFtLauderdale'\n",
      " 'Midsouth' 'Nashville' 'NewOrleansMobile' 'NewYork' 'Northeast'\n",
      " 'NorthernNewEngland' 'Orlando' 'Philadelphia' 'PhoenixTucson'\n",
      " 'Pittsburgh' 'Plains' 'Portland' 'RaleighGreensboro' 'RichmondNorfolk'\n",
      " 'Roanoke' 'Sacramento' 'SanDiego' 'SanFrancisco' 'Seattle'\n",
      " 'SouthCarolina' 'SouthCentral' 'Southeast' 'Spokane' 'StLouis' 'Syracuse'\n",
      " 'Tampa' 'TotalUS' 'West' 'WestTexNewMexico']\n"
     ]
    }
   ],
   "source": [
    "# Display the unique region names in the dataset\n",
    "print(\"Unique regions:\", df[\"region\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date = pd.Timestamp(\"2015-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44655461.51"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"region == 'TotalUS' and type == 'conventional' and Date == @date\")[\"Total Volume\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72836765.58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"region != 'TotalUS' and type == 'conventional' and Date == @date\")[\"Total Volume\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region column includes names that refer to individual cities such as Albany and Boston and also to larger areas such as California and West. There appears to be a hierarchical structure in the data. TotalUS is an aggregate of all regions and it is split into bigger areas such as West, Southeast, Northeast, and Midsouth. Moreover, California is also further divided into cities such as Sacramento, SanDiego, and LosAngeles. This indicates that while each region label is distinct in the dataset, many of these labels overlap in their geographic coverage. For example, on 2015-02-01 the volume recorded for TotalUS was 44565461.51 while the sum of the volumes for other regions was 72836756.58. The fact that these two numbers are close in scale but not identical suggests that TotalUS aggregates data from the individual regions which shows that the regions are not entirely separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from [Lecture 19](https://github.com/UBC-CS/cpsc330-2024W2/tree/main/lectures), with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag, groupby, new_feature_name=None, clip=False):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "    \n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "    \n",
    "    new_df = df.assign(**{new_feature_name : np.nan})\n",
    "    for name, group in new_df.groupby(groupby):        \n",
    "        if lag < 0: # take values from the past\n",
    "            new_df.loc[group.index[-lag:],new_feature_name] = group.iloc[:lag][orig_feature].values\n",
    "        else:       # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][orig_feature].values\n",
    "            \n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "18248 2018-03-25          1.62      15303.40  2325.30   2171.66    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "18248    10806.44    10569.80      236.64          0.0       organic  2018   \n",
       "\n",
       "                 region  \n",
       "0                Albany  \n",
       "1                Albany  \n",
       "2                Albany  \n",
       "3                Albany  \n",
       "4                Albany  \n",
       "...                 ...  \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18243 2018-02-18          1.56      17597.12  1892.05   1928.36    0.00   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18243    13776.71    13553.53      223.18          0.0       organic  2018   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0                Albany                  1.24  \n",
       "1                Albany                  1.17  \n",
       "2                Albany                  1.06  \n",
       "3                Albany                  0.99  \n",
       "4                Albany                  0.99  \n",
       "...                 ...                   ...  \n",
       "18243  WestTexNewMexico                  1.57  \n",
       "18244  WestTexNewMexico                  1.54  \n",
       "18245  WestTexNewMexico                  1.56  \n",
       "18246  WestTexNewMexico                  1.56  \n",
       "18247  WestTexNewMexico                  1.62  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 `AveragePrice` baseline \n",
    "rubric={points}\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285800937261841"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_r2 = r2_score(df_train[\"AveragePriceNextWeek\"], df_train[\"AveragePrice\"])\n",
    "train_r2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631780188583048"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_r2  = r2_score(df_test[\"AveragePriceNextWeek\"], df_test[\"AveragePrice\"])\n",
    "test_r2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model predicts that next week's average price is the same as this week's price. This simple strategy yields a train $R^2$ of about 0.83 and a test $R^2$ of around 0.76.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert not train_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert not test_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert sha1(str(round(train_r2, 3)).encode('utf8')).hexdigest() == 'b1136fe2a8918904393ab6f40bfb3f38eac5fc39', \"Your training score is not correct. Are you using the right features?\"\n",
    "assert sha1(str(round(test_r2, 3)).encode('utf8')).hexdigest() == 'cc24d9a9b567b491a56b42f7adc582f2eefa5907', \"Your test score is not correct. Are you using the right features?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "rubric={points:10}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. I got to 0.80, but not beyond that. Let me know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "def preprocess_data(df_train, df_test, num_feats, cat_feats, keep_feats, drop_feats, target_col):\n",
    "    # Reorder columns\n",
    "    order = num_feats + cat_feats + keep_feats + drop_feats + [target_col]\n",
    "    df_train = df_train[order]\n",
    "    df_test  = df_test[order]\n",
    "    \n",
    "    # Build pipelines for numeric and categorical features\n",
    "    num_pipe = Pipeline([\n",
    "        ('imp', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    transformer = ColumnTransformer([\n",
    "        ('num', num_pipe, num_feats),\n",
    "        ('cat', cat_pipe, cat_feats),\n",
    "        ('keep', 'passthrough', keep_feats),\n",
    "        ('drop', 'drop', drop_feats + [target_col])\n",
    "    ])\n",
    "    \n",
    "    transformer.fit(df_train)\n",
    "    \n",
    "    # Get new feature names for categorical data\n",
    "    ohe = transformer.named_transformers_['cat'].named_steps['ohe']\n",
    "    cat_feature_names = list(ohe.get_feature_names_out(cat_feats))\n",
    "    new_cols = num_feats + cat_feature_names + keep_feats\n",
    "    \n",
    "    # Transform data \n",
    "    X_train = pd.DataFrame(transformer.transform(df_train).toarray(), index=df_train.index, columns=new_cols)\n",
    "    X_test  = pd.DataFrame(transformer.transform(df_test).toarray(), index=df_test.index, columns=new_cols)\n",
    "    y_train = df_train[target_col]\n",
    "    y_test  = df_test[target_col]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime \n",
    "df_train.loc[:, \"Date\"] = pd.to_datetime(df_train[\"Date\"])\n",
    "df_test.loc[:, \"Date\"]  = pd.to_datetime(df_test[\"Date\"])\n",
    "df_train.loc[:, \"year\"] = df_train[\"Date\"].dt.year\n",
    "df_test.loc[:, \"year\"]  = df_test[\"Date\"].dt.year\n",
    "\n",
    "# Add Month as a categorical variable \n",
    "df_train = df_train.assign(Month = df_train[\"Date\"].dt.month.astype(str))\n",
    "df_test  = df_test.assign(Month  = df_test[\"Date\"].dt.month.astype(str))\n",
    "\n",
    "# Type and region are strings\n",
    "df_train.loc[:, \"type\"] = df_train[\"type\"].astype(str)\n",
    "df_train.loc[:, \"region\"] = df_train[\"region\"].astype(str)\n",
    "df_test.loc[:, \"type\"] = df_test[\"type\"].astype(str)\n",
    "df_test.loc[:, \"region\"] = df_test[\"region\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for Experiment 1\n",
    "num_feats1    = [\"Total Volume\", \"4046\", \"4225\", \"4770\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\", \"year\"]\n",
    "cat_feats1    = [\"type\", \"region\", \"Month\"]  # Month as categorical\n",
    "keep_feats1   = [\"AveragePrice\"]\n",
    "drop_feats1   = [\"Date\", \"Total Bags\"]\n",
    "target_col    = \"AveragePriceNextWeek\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "X_train1, y_train1, X_test1, y_test1 = preprocess_data(df_train, df_test, \n",
    "                                                        num_feats1, \n",
    "                                                        cat_feats1, \n",
    "                                                        keep_feats1, \n",
    "                                                        drop_feats1, \n",
    "                                                        target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Experiment 1: Month Encoding-----\n",
      "Ridge Best Params: {'alpha': 13}\n",
      "Ridge Best CV R^2: 0.8209426304102811\n",
      "Ridge Train R^2: 0.8492502671064784\n",
      "Ridge Test R^2: 0.8005850197548069\n"
     ]
    }
   ],
   "source": [
    "# TimeSeriesSplit and grid search for Ridge Regression\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ridge_grid = {'alpha': [11, 12, 13]}\n",
    "ridge_model = Ridge()\n",
    "ridge_gs = GridSearchCV(ridge_model, ridge_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "ridge_gs.fit(X_train1, y_train1)\n",
    "\n",
    "print(\"-----Experiment 1: Month Encoding-----\")\n",
    "print(\"Ridge Best Params:\", ridge_gs.best_params_)\n",
    "print(\"Ridge Best CV R^2:\", ridge_gs.best_score_)\n",
    "print(\"Ridge Train R^2:\", ridge_gs.score(X_train1, y_train1))\n",
    "print(\"Ridge Test R^2:\", ridge_gs.score(X_test1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Best Params: {'max_depth': 6, 'min_samples_leaf': 4, 'n_estimators': 160}\n",
      "RandomForest Best CV R^2: 0.8300186641979901\n",
      "RandomForest Train R^2: 0.857863489092283\n",
      "RandomForest Test R^2: 0.7832833193077353\n"
     ]
    }
   ],
   "source": [
    "# TimeSeriesSplit and grid search for Random Forest\n",
    "rf_grid = {\n",
    "    'n_estimators': [155, 160, 165],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'min_samples_leaf': [3, 4, 5]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=30)\n",
    "rf_gs = GridSearchCV(rf_model, rf_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "rf_gs.fit(X_train1, y_train1)\n",
    "\n",
    "print(\"RandomForest Best Params:\", rf_gs.best_params_)\n",
    "print(\"RandomForest Best CV R^2:\", rf_gs.best_score_)\n",
    "print(\"RandomForest Train R^2:\", rf_gs.score(X_train1, y_train1))\n",
    "print(\"RandomForest Test R^2:\", rf_gs.score(X_test1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Coefficients (Experiment 1):\n",
      "                Feature  Coefficient   AbsCoef\n",
      "73         AveragePrice     0.769744  0.769744\n",
      "8          type_organic     0.115029  0.115029\n",
      "26       region_Houston    -0.094677  0.094677\n",
      "72              Month_9     0.093831  0.093831\n",
      "50  region_SanFrancisco     0.086949  0.086949\n",
      "..                  ...          ...       ...\n",
      "3                  4770    -0.004102  0.004102\n",
      "43        region_Plains    -0.003089  0.003089\n",
      "55       region_Spokane    -0.002549  0.002549\n",
      "51       region_Seattle    -0.000752  0.000752\n",
      "0          Total Volume     0.000349  0.000349\n",
      "\n",
      "[74 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print Ridge coefficients \n",
    "ridge_best1 = ridge_gs.best_estimator_\n",
    "coef_df1 = pd.DataFrame({\n",
    "    'Feature': X_train1.columns,\n",
    "    'Coefficient': ridge_best1.coef_\n",
    "})\n",
    "\n",
    "coef_df1['AbsCoef'] = coef_df1['Coefficient'].abs()\n",
    "print(\"Ridge Coefficients (Experiment 1):\")\n",
    "print(coef_df1.sort_values(\"AbsCoef\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Experiment 1 with month encoding, the Ridge regression model achieved a CV $R^2$ of 0.82, with training and test \\( R^2 \\) scores respectively of **0.85** and **0.80** and had best parameter: alpha = 13. The Random Forest model achieved a CV $R^2$ of **0.83**, with training and test scores of respectively **0.86** and **0.78** and had best parameters: max_depth=6, min_samples_leaf=4, and n_estimators=160. The Ridge coefficients indicated that the most influential feature was AveragePrice (0.77), followed by type_organic (0.12) and region_Houston (-0.09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define feature sets for Experiment 2\n",
    "num_feats2    = [\"Total Volume\", \"4046\", \"4225\", \"4770\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"]  # Remove year\n",
    "cat_feats2    = [\"type\", \"region\", \"year\"]  # 'year' as categorical\n",
    "keep_feats2   = [\"AveragePrice\"]\n",
    "drop_feats2   = [\"Date\", \"Total Bags\", \"Month\"]  # Drop Month in this experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junse\\miniconda3\\envs\\cpsc330\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X_train2, y_train2, X_test2, y_test2 = preprocess_data(df_train, df_test, \n",
    "                                                        num_feats2, \n",
    "                                                        cat_feats2, \n",
    "                                                        keep_feats2, \n",
    "                                                        drop_feats2, \n",
    "                                                        target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Experiment 2: Year Encoding-----\n",
      "Ridge Best Params: {'alpha': 13}\n",
      "Ridge Best CV R^2: 0.8194356349664481\n",
      "Ridge Train R^2: 0.8458904402462581\n",
      "Ridge Test R^2: 0.7971467674345594\n"
     ]
    }
   ],
   "source": [
    "# TimeSeriesSplit and grid search for Ridge Regression\n",
    "ridge_model2 = Ridge()\n",
    "ridge_gs2 = GridSearchCV(ridge_model2, ridge_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "ridge_gs2.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"\\n-----Experiment 2: Year Encoding-----\")\n",
    "print(\"Ridge Best Params:\", ridge_gs2.best_params_)\n",
    "print(\"Ridge Best CV R^2:\", ridge_gs2.best_score_)\n",
    "print(\"Ridge Train R^2:\", ridge_gs2.score(X_train2, y_train2))\n",
    "print(\"Ridge Test R^2:\", ridge_gs2.score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Best Params: {'max_depth': 5, 'min_samples_leaf': 4, 'n_estimators': 155}\n",
      "RandomForest Best CV R^2: 0.829303431930728\n",
      "RandomForest Train R^2: 0.8486844531578566\n",
      "RandomForest Test R^2: 0.7831178192600694\n"
     ]
    }
   ],
   "source": [
    "# TimeSeriesSplit and grid search for Random Forest\n",
    "rf_model2 = RandomForestRegressor(random_state=30)\n",
    "rf_gs2 = GridSearchCV(rf_model2, rf_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "rf_gs2.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"RandomForest Best Params:\", rf_gs2.best_params_)\n",
    "print(\"RandomForest Best CV R^2:\", rf_gs2.best_score_)\n",
    "print(\"RandomForest Train R^2:\", rf_gs2.score(X_train2, y_train2))\n",
    "print(\"RandomForest Test R^2:\", rf_gs2.score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Coefficients (Experiment 2):\n",
      "                       Feature  Coefficient   AbsCoef\n",
      "63                AveragePrice     0.790035  0.790035\n",
      "7                 type_organic     0.104783  0.104783\n",
      "25              region_Houston    -0.086112  0.086112\n",
      "49         region_SanFrancisco     0.079545  0.079545\n",
      "24  region_HartfordSpringfield     0.077135  0.077135\n",
      "..                         ...          ...       ...\n",
      "1                         4046     0.003073  0.003073\n",
      "42               region_Plains    -0.002671  0.002671\n",
      "54              region_Spokane    -0.001854  0.001854\n",
      "0                 Total Volume    -0.000095  0.000095\n",
      "50              region_Seattle    -0.000042  0.000042\n",
      "\n",
      "[64 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print Ridge coefficients for inspection\n",
    "ridge_best2 = ridge_gs2.best_estimator_\n",
    "coef_df2 = pd.DataFrame({\n",
    "    'Feature': X_train2.columns,\n",
    "    'Coefficient': ridge_best2.coef_\n",
    "})\n",
    "\n",
    "coef_df2['AbsCoef'] = coef_df2['Coefficient'].abs()\n",
    "print(\"Ridge Coefficients (Experiment 2):\")\n",
    "print(coef_df2.sort_values(\"AbsCoef\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Experiment 2 with year encoding, the Ridge regression model achieved a CV $R^2$ of 0.82, with training and test $R^2$ scores respectively of **0.85** and **0.80** and had best parameter: alpha = 13. The Random Forest model achieved a CV $R^2$ of **0.83**, with training and test scores respectively of **0.85** and **0.78** and had best parameters: max_depth=5, min_samples_leaf=4, and n_estimators=155. The Ridge coefficients showed the most influential feature was AveragePrice (0.79), followed by type_organic (0.10) and region_Houston (-0.09).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "I compared two approaches for encoding the date in our avocado price forecast. In Experiment 1, I used month encoding by making the month as a categorical variable to capture seasonal patterns in price fluctuations. In Experiment 2, I encoded the year as a categorical variable to reflect broader annual trends. The results showed that experiment 1 slightly perform better compared to the experiment 2 for the month encoding approach. This suggests that month encoding captures seasonal variations more accurately. In conclusion, month encoding appeared more effective for forecasting next week's avocado prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 20 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer.\n",
    "3. When studying time series modeling, we explored several ways to encode date information as a feature for the citibike dataset. When we used time of day as a numeric feature, the Ridge model was not able to capture the periodic pattern. Why? How did we tackle this problem? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real-world situation where time series data would have unequally spaced time points is the recording of emergency service calls. Emergency service calls occur unpredictably and randomly. Therefore, this leads to irregularly spaced data points rather than fixed intervals.\n",
    "\n",
    "Creating lagged versions of features struggles with unequally spaced time points. Lagged features assume there is a constant interval between observations. When the intervals vary, these lagged features do not represent consistent durations, thus misleading the model. On the other hand, encoding the date as one or more features provides more flexibility in adapting to irregular spacing. Therefore, lagged features struggle with unequally spaced time points.\n",
    "\n",
    "When the time of day was encoded as a numeric value, the Ridge model could not capture the periodic behavior inherent in the data. A numeric representation does not reflect the cyclic nature of a day. For example, 23:00 and 01:00 are close in terms of actual time, even though their numeric values appear far apart. To overcome this problem, we transformed the time of day into a categorical variable using one-hot encoding. This encoding allowed the model to learn independent effects for each time category and powerfully capturing the cyclic pattern observed in the citibike dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Computer vision \n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 19 on multiclass classification and introduction to computer vision. \n",
    "\n",
    "1. How many parameters (coefficients and intercepts) will `sklearn`’s `LogisticRegression()` model learn for a four-class classification problem, assuming that you have 10 features? Briefly explain your answer.\n",
    "2. In Lecture 19, we briefly discussed how neural networks are sort of like `sklearn`'s pipelines, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?\n",
    "3. Imagine that you have a small dataset with ~1000 images containing pictures and names of 50 different Computer Science faculty members from UBC. Your goal is to develop a reasonably accurate multi-class classification model for this task. Describe which model/technique you would use and briefly justify your choice in one to three sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each class the logistic regression learns one coefficient for every feature and one intercept. For a four-class classification problem, assuming that you have 10 features, the model learns a total of 44 parameters because 4 times 10 plus 4 is 44.\n",
    "\n",
    "2. Neural networks perform several sequential transformations on the data in a manner similar to a pipeline and this allows the early layers to serve as general feature extractors. This property is useful because it enables one to reuse pre-existing representations from a network trained on a large dataset and then adapt the later layers to the specific new task.\n",
    "\n",
    "3. I would use a pre-trained convolutional neural network like AlexNet and fine tune its final classification layer. This technique is a good choice because the network has already learned robust image features from a large dataset and only the last layers need to be adjusted to the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.3 Survival analysis\n",
    "<hr>\n",
    "\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 21 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer? Briefly explain your answer. \n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There is a problem with imply labeling customers are \"churned\" or \"not churned\" because assigning a binary label fails to capture that customers churn at different times. This standard supervised learning techniques do not consider that many customers are still active at the time of data collection. In doing so, it mistakenly assumes these customers will remain indefinitely. So, this method does not reflect accurately about the true duration of customer engagement.\n",
    "\n",
    "2. We would expect that customer A who just joined is more likely to leave sooner than customer B. New customers are typically in the period where many users drop out while those who have been with the service for a longer time have already demonstrated their willingness to remain. Therefore, they are at a lower risk during the early phase.\n",
    "\n",
    "3. If a customer’s survival function is almost flat during a certain period, it indicates that the probability of the customer staying with the service remains nearly constant, suggesting that the risk of churn is very low throughout that time. To put it simply, few additional customers are leaving during this period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "name": "_merged",
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
